{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D7041E Lab2 Noah Oriano #\n",
    "noaori-4@student.ltu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Word 2 Vector Results ##\n",
    "### Dimension = 10 ###\n",
    "#### First Run ####\n",
    "Each training EPOCH took about 11 seconds\n",
    "Threshold ferq = 0.00055; Percentage of correct answers: 47.5%\n",
    "#### Second Run ####\n",
    "Each training EPOCH took about 30 seconds IN BACKGROUND\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 48.75%\n",
    "#### Third Run ####\n",
    "Each training EPOCH took about 33 seconds IN BACKGROUND\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 50.0%\n",
    "#### Fourth Run ####\n",
    "Each training EPOCH took about 10.5 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 48.75%\n",
    "#### Fifth Run ####\n",
    "Each training EPOCH took about 11 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 51.25%\n",
    "\n",
    "### Dimension = 30 ###\n",
    "#### First Run ####\n",
    "Each training EPOCH took about 11 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 63.75%\n",
    "#### Second Run ####\n",
    "Each training EPOCH took about 10 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 66.25%\n",
    "#### Third Run ####\n",
    "Each training EPOCH took about 30 seconds (typing but still inside IDE, unsure what slowed it down)\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 65.0%\n",
    "#### Fourth Run ####\n",
    "Each training EPOCH took about 10 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 63.75%\n",
    "#### Fifth Run ####\n",
    "Each training EPOCH took about 10.5 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 66.25%\n",
    "\n",
    "### Dimension = 100 ###\n",
    "#### First Run ####\n",
    "Each training EPOCH took about 19 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 70.0%\n",
    "#### Second Run ####\n",
    "Each training EPOCH took between 20 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 70.0%\n",
    "#### Third Run ####\n",
    "Each training EPOCH took about 20 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 67.5%\n",
    "#### Fourth Run ####\n",
    "Each training EPOCH took about 40 seconds IN BACKGROUND\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 61.25%\n",
    "#### Fifth Run ####\n",
    "Each training EPOCH took about 16 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 68.75%\n",
    "\n",
    "### Dimension = 1000 ###\n",
    "#### First Run ####\n",
    "Each training EPOCH took about 115 seconds (background)\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 68.75%\n",
    "#### Second Run ####\n",
    "Each training EPOCH took about 110 seconds (background)\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 70.0%\n",
    "#### Third Run ####\n",
    "Each training EPOCH took about 33 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 72.5%\n",
    "#### Fourth Run ####\n",
    "Each training EPOCH took about 35 seconds\n",
    "Threshold ferq = 0.00055 Percentage of correct answers: 67.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Word 2 Vector Analysis ##\n",
    "Loading time/collection was very fast, mostly negligable. All of the training took a reasonable amount of time, that scaled with dimension, but scaled rather slowly and non-linearly. All training was with 5 Epochs (0-4), so the total time varied from around 50 to 150 seconds while in focus, and 150 to 550 seconds while in the backround.\n",
    "The accuracy of the Word2Vec increased from 10 to 30, increased slightly from 30 to 100, and increased only around 2.5% from 100 to 1000. The improvement of accuracy due to the increase in dimension falls off very quickly. I would expect very little to no improvement with a higher dimensionality than 1000.\n",
    "There is somewhat significant random variance in the accuracy. Values seem to vary from the average around 2.5%, but in one instance with dimension = 100 there was a 8.75% difference from max and minimum found accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RI Results ##\n",
    "### Dimension = 1000 ###\n",
    "#### Run 1 ####\n",
    "1000 Percentage of correct answers: 66.25%\n",
    "Time: 23.89 seconds\n",
    "#### Run 2 ####\n",
    "1000 Percentage of correct answers: 73.75%\n",
    "Time: 23.77 seconds\n",
    "#### Run 3 ####\n",
    "1000 Percentage of correct answers: 70.0%\n",
    "Time: 25.11 seconds\n",
    "#### Run 4 ####\n",
    "1000 Percentage of correct answers: 75.0%\n",
    "Time: 24.46 seconds\n",
    "#### Run 5 ####\n",
    "1000 Percentage of correct answers: 75.0%\n",
    "Time: 23.87 seconds\n",
    "#### Run 6 ####\n",
    "1000 Percentage of correct answers: 65.0%\n",
    "Time: 23.64 seconds\n",
    "Estimated time for the whole corpus: 4334.82 seconds\n",
    "\n",
    "### Dimension = 4000 ###\n",
    "#### Run 1 ####\n",
    "Percentage of correct answers: 80.0%\n",
    "Time: 29.33601188659668\n",
    "Estimated time for the whole corpus: 5378.461384125499 seconds\n",
    "#### Run 2 ####\n",
    "Percentage of correct answers: 78.75%\n",
    "Time: 29.59960436820984\n",
    "Estimated time for the whole corpus: 5426.731282351234 seconds\n",
    "#### Run 3 ####\n",
    "Percentage of correct answers: 71.25%\n",
    "Time: 29.446288347244263\n",
    "Estimated time for the whole corpus: 5398.553236160031 seconds\n",
    "#### Run 4 ####\n",
    "Percentage of correct answers: 71.25%\n",
    "Time: 29.995988130569458\n",
    "Estimated time for the whole corpus: 5499.423237947365 seconds\n",
    "#### Run 5 ####\n",
    "Percentage of correct answers: 68.75%\n",
    "Time: 30.400614261627197\n",
    "Estimated time for the whole corpus: 5573.602946266571 seconds\n",
    "#### Run 6 ####\n",
    "Percentage of correct answers: 72.5%\n",
    "Time: 29.616523504257202\n",
    "Estimated time for the whole corpus: 5429.803605151486 seconds\n",
    "#### Run 7 ####\n",
    "Percentage of correct answers: 71.25%\n",
    "Time: 29.641133546829224\n",
    "Estimated time for the whole corpus: 5434.366871495061 seconds\n",
    "\n",
    "### Dimension = 10000 ###\n",
    "#### Run 1 ####\n",
    "Percentage of correct answers: 70.0%\n",
    "Time: 45.3015353679657\n",
    "Estimated time for the whole corpus: 8305.42719846329 seconds\n",
    "#### Run 2 ####\n",
    "Percentage of correct answers: 73.75%\n",
    "Time: 44.84660768508911\n",
    "Estimated time for the whole corpus: 8221.932655352432 seconds\n",
    "#### Run 3 ####\n",
    "Percentage of correct answers: 75.0%\n",
    "Time: 45.876776456832886\n",
    "Estimated time for the whole corpus: 8410.88594944632 seconds\n",
    "#### Run 4 ####\n",
    "Percentage of correct answers: 73.75%\n",
    "Time: 45.70019841194153\n",
    "Estimated time for the whole corpus: 8378.512193178201 seconds\n",
    "#### Run 5 ####\n",
    "Percentage of correct answers: 71.25%\n",
    "Time: 45.11758041381836\n",
    "Estimated time for the whole corpus: 8271.700621829714 seconds\n",
    "#### Run 6 ####\n",
    "Percentage of correct answers: 73.75%\n",
    "Time: 44.680925607681274\n",
    "Estimated time for the whole corpus: 8191.464343290824 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RI Code Problem ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the code is only adequetly working when window size is 2 ###\n",
    "The issue with the code related to window size is that the edge cases are hard coded for the assumption of there only being a window size of two. (lines 96-134)\n",
    "For example, if there were no words to the left of a word on the same line, the edge cases do not account for larger window sizes, and will treat the updating of embeddings as if the window size was two.\n",
    "### To fix the code to allow window sizes that are not 2 ###\n",
    "The handling of words outside of the same line should be handled within the loop that handles most of the neighbor processing. This would make the code more dynamic. The code should look something like the following code for the left neighbors, and would look similar (with the different values and indexing) for the right neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "# Process \n",
    "while (k <= window_size): # process left neighbors of the focus word\n",
    "    if (i - k) >= 0:\n",
    "        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i - k]], -1))\n",
    "    else:\n",
    "        if (i - k) == -1:       \n",
    "            word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[1][0]], -1))\n",
    "        else: # (i - k) < -1\n",
    "            word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[0][len(lines[0]) + (i-k+1)]], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above essentially handles words on the same line as before, but then when the line end is detected, the behavior from the previous code for the end of the line is ran once if necesary (gets the first word in the lines[1]), and then the words from lines[0] are used from last to first until k is greater than the window size. This was implemented by moving the (i-k >= 0) from a while loop condition to an if statement. Instead of breaking the while loop, different logic is ran in the loop. This keeps the code very dynamic, although it is still limited to only reaching one line to the left, further code rework is necessary to have very large window sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of results of RI ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy seemed to increase slightly from 1,000 to 4,000 dimensionality, but didn't increase noticably from 4,000 to 10,000. The run time also increased slightly, from about 24 seconds to 45 seconds. The estimated time for the entire corpus was above 8300 seconds for 10k dimensionality. This is about 2 1/3 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparason of W2V and RI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The W2V and RI performed at a similar accuracy level for dimensionality of 1000 (around 70%). The RI had a run time about 6 times faster, however, it only ran on a subset of the total words. The estimated time for the entire corpus with RI was around 4,300 seconds, much higher than the W2V (29 times slower). RI was ran for higher dimensionalities with only minor increases in accuracy, so the models overall seem to have similar traits with accuracy vs dimensionality, while RI would be much slower if ran on the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results from the time for the corpus were gained by taking the real time * unique corpus words / unique TOEFL words. From my code, the following stats were determined for that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total unique words in the corpus: 70583.\n",
    "Total unique words in the TOEFL dataset: 385"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
