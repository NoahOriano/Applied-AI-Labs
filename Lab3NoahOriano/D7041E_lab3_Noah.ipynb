{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noah Oriano, noaori-4@student.ltu.se, 040114-T130 ###\n",
    "### D7041E Lab 3 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are general function definitions and global variables related to the gathering of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.1 Import datasets into Jupyter environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import unidecode\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "# I have manually downloaded and stored the 21 datasets (30k size) in the news_datasets folder.\n",
    "# I will load these datasets, preprocess them and store them as numpy files.\n",
    "# I will store these numpy variables in seperate files. This is just to make this code more flexible if I want to use larger datasets.\n",
    "\n",
    "# First, I will define some funtions for gathering the data\n",
    "base_folder = 'news_datasets_30k'\n",
    "base_path = 'news_datasets_30k/'\n",
    "processed_path = 'processed_datasets_30k/'\n",
    "reload_data = True\n",
    "sample_size = 1000\n",
    "# Use n=3 (tri-grams). Use length of HD vectors d1=100 and d2=1000.\n",
    "symbol_size = 3\n",
    "d1 = 100\n",
    "d2 = 1000\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz ' # 27 characters\n",
    "# The name of the dataset is the dataset folder's first word in its name\n",
    "def get_dataset_name(dataset_folder):\n",
    "    return dataset_folder.split('_')[0]\n",
    "# Returns the dataset folder path without base path, base path must be included individually\n",
    "def get_dataset_folder_paths():\n",
    "    paths = glob.glob(base_path + '*/')\n",
    "    paths = [path.replace(base_folder + \"\\\\\", '') for path in paths]\n",
    "    return paths\n",
    "\n",
    "def process_data(data):\n",
    "    # Decode the data to remove accents and other special characters and convert them to their closest latin\n",
    "    data = unidecode.unidecode(data)\n",
    "    # Randomize order of sentences\n",
    "    data = data.split('\\n')\n",
    "    np.random.shuffle(data)\n",
    "    data = '\\n'.join(data)\n",
    "    # Convert the data so that it only contains values in the alphabet\n",
    "    data = data.lower()\n",
    "    data = ''.join(e for e in data if e in alphabet)\n",
    "    # When we have multiple spaces, we will replace them with a single space\n",
    "    data = ' '.join(data.split())\n",
    "    # Convert data into numpy array\n",
    "    data = np.array(list(data))\n",
    "    return data\n",
    "\n",
    "# Returns data as a dictionary with dataset names as keys and their data as values\n",
    "def get_data_from_folder(folder_path):\n",
    "    paths = glob.glob(base_path + folder_path + '/*.txt') # Only text files, not the sql files\n",
    "    language_data = {}\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding = \"utf-8\") as file:\n",
    "            name = path.split('-')[-1].split('.')[0] # Example : pol_news_2023_30K\\pol_news_2023_30K-inv_so.txt -> inv_so\n",
    "            # Read data in UTF-8 format\n",
    "            temp_data = file.read()\n",
    "            language_data[name] = process_data(temp_data)\n",
    "    return language_data\n",
    "\n",
    "# Save the data as numpy files\n",
    "def save_data_as_numpy(language_data, folder_path):\n",
    "    # Check if the folder exists, if not create it\n",
    "    if not os.path.exists(processed_path + folder_path):\n",
    "        os.makedirs(processed_path + folder_path)\n",
    "    for key in language_data.keys():\n",
    "        np.save(processed_path + folder_path + key, language_data[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code uses the above code and is just for preprocessing data into the folders. \n",
    "This is seperate so it does not have to be rerun with following code that handles more complex functionality. The cobined data variable can also just be used in later code with the current setup since the data is not memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset folder paths are : ['bul_news_2022_30K\\\\', 'ces_news_2023_30K\\\\', 'dan_news_2022_30K\\\\', 'deu_news_2023_30K\\\\', 'ell_news_2023_30K\\\\', 'eng_news_2023_30K\\\\', 'est_news_2022_30K\\\\', 'fin_news_2022_30K\\\\', 'fra_news_2023_30K\\\\', 'hun_news_2023_30K\\\\', 'ita_news_2023_30K\\\\', 'lav_news_2020_30K\\\\', 'lit_news_2020_30K\\\\', 'nld_news_2023_30K\\\\', 'pol_news_2023_30K\\\\', 'por_news_2023_30K\\\\', 'ron_news_2022_30K\\\\', 'slk_news_2020_30K\\\\', 'slv_news_2020_30K\\\\', 'spa_news_2023_30K\\\\', 'swe_news_2023_30K\\\\']\n",
      "All data is saved as numpy files in the processed_datasets folder.\n",
      "The data is ready to be used in the next step.\n",
      "The combined data is saved as a numpy file.\n"
     ]
    }
   ],
   "source": [
    "if(reload_data):\n",
    "    # First, get the dataset folder paths\n",
    "    dataset_folder_paths = get_dataset_folder_paths()\n",
    "    print('The dataset folder paths are :', dataset_folder_paths)\n",
    "    # Get the data from the folders\n",
    "    combined_data = {}\n",
    "    for folder_path in dataset_folder_paths:\n",
    "        language_data = get_data_from_folder(folder_path)\n",
    "        classifier_name = get_dataset_name(folder_path)\n",
    "        combined_data[classifier_name] = language_data\n",
    "        save_data_as_numpy(language_data, folder_path)\n",
    "    print('All data is saved as numpy files in the processed_datasets folder.')\n",
    "    print('The data is ready to be used in the next step.')\n",
    "\n",
    "    # Save the combined data as a numpy file\n",
    "    np.save('combined_data', combined_data)\n",
    "    print('The combined data is saved as a numpy file.')\n",
    "    # Data will be stored as a dictionary within a dictionary. \n",
    "    # The first dictionary is classifier names and the second dictionary is the dataset names.\n",
    "else:\n",
    "    combined_data = np.load('combined_data.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bul', 'ces', 'dan', 'deu', 'ell', 'eng', 'est', 'fin', 'fra', 'hun', 'ita', 'lav', 'lit', 'nld', 'pol', 'por', 'ron', 'slk', 'slv', 'spa', 'swe'])\n",
      "dict_keys(['co_n', 'co_s', 'inv_so', 'inv_w', 'meta', 'sentences', 'sources', 'words'])\n",
      "(2713780,)\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.keys())\n",
    "print(combined_data['pol'].keys())\n",
    "print(combined_data['pol']['sentences'].shape)\n",
    "print(combined_data['pol']['sentences'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the data into samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d' 'z' 'i']\n"
     ]
    }
   ],
   "source": [
    "# Rather than using sentences, the classifier will use \"symbols\"\n",
    "number_of_languages = len(combined_data.keys())\n",
    "initial_data_sets = []\n",
    "for key in combined_data.keys():\n",
    "    data_to_add = combined_data[key]['sentences']\n",
    "    # Convert the data into n-gram symbols\n",
    "    data_to_add = [data_to_add[(i*symbol_size):((i+1)*symbol_size)] for i in range(len(data_to_add)//symbol_size)]\n",
    "    if(key == 'pol'):\n",
    "        print(data_to_add[0])\n",
    "    initial_data_sets.append((data_to_add, key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l' 'i' 'p']\n",
      "lip\n",
      "bul\n"
     ]
    }
   ],
   "source": [
    "print(initial_data_sets[0][0][0])\n",
    "\n",
    "# Now, to get the paired data we need to convert the data into strings instead of nparrays\n",
    "paired_data_sets = []\n",
    "for data, key in initial_data_sets:\n",
    "    data = [''.join(data[i]) for i in range(len(data))]\n",
    "    paired_data_sets.append((data, key))\n",
    "print(paired_data_sets[0][0][0])\n",
    "print(paired_data_sets[0][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "2\n",
      "1094079\n",
      "3\n",
      "bul\n",
      "['lip', 'sa ', 'na ', 'leg', 'la ']\n",
      "sa \n"
     ]
    }
   ],
   "source": [
    "print(len(paired_data_sets))\n",
    "print(len(paired_data_sets[0]))\n",
    "print(len(paired_data_sets[0][0]))\n",
    "print(len(paired_data_sets[0][0][0]))\n",
    "print(paired_data_sets[0][1])\n",
    "print(paired_data_sets[0][0][0:5])\n",
    "print(paired_data_sets[0][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the definitions for functions and test data segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(v):\n",
    "    value = np.sqrt((np.dot(v, v)) + 0.001)\n",
    "    return value if value else 0.001\n",
    "\n",
    "# Generate high-dimensional vectors\n",
    "def generate_hd_vector(d):\n",
    "    return np.random.choice([-1, 1], size=d)\n",
    "\n",
    "# Encode n_gram using character mappings\n",
    "def encode_n_gram(n_gram, char_to_hd, dim):\n",
    "    default_vector = generate_hd_vector(dim)  \n",
    "    return np.sum([char_to_hd.get(char, default_vector) for char in n_gram], axis=0)\n",
    "\n",
    "# Compute language centroid\n",
    "def compute_language_centroid(ngram_vectors):\n",
    "    centroid = np.sum(ngram_vectors, axis=0)\n",
    "    return centroid / (norm(centroid) + 0.01)\n",
    "\n",
    "# Check the cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "# Classify a query vector against centroids\n",
    "def classify_text(query_vector, centroids):\n",
    "    similarities = {language: cosine_similarity(query_vector, centroid) for language, centroid in centroids.items()}\n",
    "    return max(similarities, key=similarities.get)\n",
    "\n",
    "# Generate encoded n-grams for a text\n",
    "def generate_encoded_ngrams(sample, n, d, char_to_hd):\n",
    "    ngram_vectors = []\n",
    "    for n_gram in sample:\n",
    "        if(len(n_gram) < n):\n",
    "            print('Skipping ngram :', n_gram)\n",
    "            continue\n",
    "        encoded = encode_n_gram(n_gram, char_to_hd, d)\n",
    "        ngram_vectors.append(encoded)\n",
    "    return ngram_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: what will be the size of the n-gram input vector in conventional (local) representation? ##\n",
    "\n",
    "The N-gram statisics woud have a size equal to A^n, where n is the number of characters in a symbol and A is the size of the alphabet. In this model, I am using tri-grams with A = 27, as there are 26 letters plus the space character. This gives 27^3 = 19683 statistics.\n",
    "\n",
    "The HD vector generated that is associated with each symbol is of size equal to the given dimension, in this case 1000.\n",
    "\n",
    "## Question: Identify difficulties of working with conventional representations of n-grams in the machine learning context.\n",
    "\n",
    "Data is sparse, some symbols appear much more often. On top of this, the feature space grows exponentially with the size of the alphabet and n. This makes n-grams somewhat un-effecient in terms of memory or computation. N-grams do not handle contextual information, since they have a small window. The cost of computation also grows quickly with dimensionality as well, and a lot of data is needed for effecient handling of larger paramaters (such as larger dimensionality and N).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22510\n",
      "1000\n",
      "2\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Split the paired data sample symbols into groups of 1000\n",
    "new_data_pairing = []\n",
    "sample_size = 1000\n",
    "index = 0\n",
    "for symbols, key in paired_data_sets:\n",
    "    language_new_pairings = []\n",
    "    for i in range(0, len(symbols), sample_size):\n",
    "        language_new_pairings.append((symbols[i:i+sample_size]))\n",
    "        paired_data_sets\n",
    "    # For replace the symbols with their symbol label pairs\n",
    "    language_new_pairings = [[(symbol, key) for symbol in sample] for sample in language_new_pairings]\n",
    "    new_data_pairing.append(language_new_pairings)\n",
    "    index += 1\n",
    "\n",
    "# Each item is now combined, now we can combine all of the languages\n",
    "combined_items = []\n",
    "for language in new_data_pairing:\n",
    "    for sample in language:\n",
    "        combined_items.append(sample)\n",
    "print(len(combined_items))\n",
    "print(len(combined_items[0]))\n",
    "print(len(combined_items[0][0]))\n",
    "print(len(combined_items[0][0][0]))\n",
    "print(len(combined_items[0][0][1]))\n",
    "\n",
    "# Now, we can create the combined samples and labels\n",
    "all_samples = []\n",
    "all_labels = []\n",
    "for sample in combined_items:\n",
    "    for symbol, label in sample:\n",
    "        all_samples.append(symbol)\n",
    "        all_labels.append(label)\n",
    "        \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_samples, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Sort the training data into bins based on language\n",
    "training_data_by_language = {}\n",
    "for(sample, label) in zip(X_train, y_train):\n",
    "    if(label not in training_data_by_language):\n",
    "        training_data_by_language[label] = []\n",
    "    training_data_by_language[label].append(sample)\n",
    "\n",
    "testing_data_by_language = {}\n",
    "for(sample, label) in zip(X_test, y_test):\n",
    "    if(label not in testing_data_by_language):\n",
    "        testing_data_by_language[label] = []\n",
    "    testing_data_by_language[label].append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "221\n",
      "1000\n",
      "3\n",
      "jet\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into samples of size 1000\n",
    "new_training_data = {}\n",
    "for key in training_data_by_language.keys():\n",
    "    for i in range(0, len(training_data_by_language[key]), sample_size):\n",
    "        if(key not in new_training_data):\n",
    "            new_training_data[key] = []\n",
    "        new_training_data[key].append((training_data_by_language[key][i:i+sample_size]))\n",
    "new_testing_data = {}\n",
    "for key in testing_data_by_language.keys():\n",
    "    for i in range(0, len(testing_data_by_language[key]), sample_size):\n",
    "        if(key not in new_testing_data):\n",
    "            new_testing_data[key] = []\n",
    "        new_testing_data[key].append((testing_data_by_language[key][i:i+sample_size]))\n",
    "\n",
    "print(len(new_testing_data))\n",
    "print(len(new_testing_data[\"slv\"]))\n",
    "print(len(new_testing_data[\"slv\"][0]))\n",
    "print(len(new_testing_data[\"slv\"][0][0]))\n",
    "print((new_testing_data[\"slv\"][0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_by_language = new_training_data\n",
    "testing_data_by_language = new_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples are of size 1000\n",
      "All samples are of size 1000\n",
      "HD Vectors generated\n",
      "swe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noaho\\AppData\\Local\\Temp\\ipykernel_26944\\2764995668.py:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  value = np.sqrt((np.dot(v, v)) + 0.001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: swe, Centroid: [nan nan nan nan nan nan nan nan nan nan]...\n",
      "hun\n",
      "HD Vectors generated\n",
      "swe\n",
      "Language: swe, Centroid: [-0.08801685 -0.35206739  0.17603369 -2.02438749 -0.88016847  0.\n",
      " -0.26405054 -3.08058966 -1.05620217 -0.96818532]...\n",
      "hun\n"
     ]
    }
   ],
   "source": [
    "# Check that all of the samples are of size 1000\n",
    "revised_training_data = {}\n",
    "for language in training_data_by_language.keys():\n",
    "    for sample in training_data_by_language[language]:\n",
    "        if(len(sample) != 1000):\n",
    "            print(len(sample))\n",
    "            print('Error')\n",
    "        else:\n",
    "            if(language not in revised_training_data):\n",
    "                revised_training_data[language] = []\n",
    "            revised_training_data[language].append(sample)\n",
    "\n",
    "errors = False\n",
    "for language in revised_training_data.keys():\n",
    "    for sample in revised_training_data[language]:\n",
    "        if(len(sample) != 1000):\n",
    "            print(len(sample))\n",
    "            print('Error2')\n",
    "            errors = True\n",
    "if(not errors):\n",
    "    print('All samples are of size 1000')\n",
    "    training_data_by_language = revised_training_data\n",
    "\n",
    "# Do the above for testing data as well\n",
    "revised_testing_data = {}\n",
    "for language in testing_data_by_language.keys():\n",
    "    for sample in testing_data_by_language[language]:\n",
    "        if(len(sample) != 1000):\n",
    "            print(len(sample))\n",
    "            print('Error')\n",
    "        else:\n",
    "            if(language not in revised_testing_data):\n",
    "                revised_testing_data[language] = []\n",
    "            revised_testing_data[language].append(sample)\n",
    "\n",
    "errors = False\n",
    "for language in revised_testing_data.keys():\n",
    "    for sample in revised_testing_data[language]:\n",
    "        if(len(sample) != 1000):\n",
    "            print(len(sample))\n",
    "            print('Error2')\n",
    "            errors = True\n",
    "if(not errors):\n",
    "    print('All samples are of size 1000')\n",
    "    testing_data_by_language = revised_testing_data\n",
    "\n",
    "# Test the generation of the centroids\n",
    "for d in [d1, d2]:\n",
    "    n = symbol_size\n",
    "    # Generate HD vectors for characters\n",
    "    char_to_hd = {char: generate_hd_vector(d) for char in alphabet}\n",
    "    print(\"HD Vectors generated\")\n",
    "\n",
    "    # Compute centroids for each language\n",
    "    language_centroids = {}\n",
    "    for language, samples in training_data_by_language.items():\n",
    "        print(language)\n",
    "        if language == \"swe\":\n",
    "            ngram_vectors = generate_encoded_ngrams(samples, n, d, char_to_hd)\n",
    "            language_centroids[language] = compute_language_centroid(ngram_vectors)\n",
    "            print(f\"Language: {language}, Centroid: {language_centroids[language][:10]}...\")  # Print first 10 elements of the centroid for brevity\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['swe', 'hun', 'ita', 'est', 'slk', 'dan', 'fin', 'lit', 'ron', 'spa', 'deu', 'pol', 'eng', 'ces', 'fra', 'bul', 'por', 'slv', 'ell', 'lav', 'nld'])\n"
     ]
    }
   ],
   "source": [
    "print(training_data_by_language.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the graph and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noaho\\AppData\\Local\\Temp\\ipykernel_26944\\2764995668.py:2: RuntimeWarning: invalid value encountered in sqrt\n",
      "  value = np.sqrt((np.dot(v, v)) + 0.001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "ita\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n",
      "Error\n",
      "est\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[1;32m---> 11\u001b[0m     ngram_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_encoded_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_to_hd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# if the vector contains NaN values, print the language and the centroid, do not include it\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39misnan(ngram_vectors)\u001b[38;5;241m.\u001b[39many()):\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mgenerate_encoded_ngrams\u001b[1;34m(sample, n, d, char_to_hd)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSkipping ngram :\u001b[39m\u001b[38;5;124m'\u001b[39m, n_gram)\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mencode_n_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_to_hd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     ngram_vectors\u001b[38;5;241m.\u001b[39mappend(encoded)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ngram_vectors\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mencode_n_gram\u001b[1;34m(n_gram, char_to_hd, dim)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_n_gram\u001b[39m(n_gram, char_to_hd, dim):\n\u001b[0;32m     11\u001b[0m     default_vector \u001b[38;5;241m=\u001b[39m generate_hd_vector(dim)  \n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchar_to_hd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn_gram\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for d in [d1, d2]:\n",
    "    n = symbol_size\n",
    "    # Generate HD vectors for characters\n",
    "    char_to_hd = {char: generate_hd_vector(d) for char in alphabet}\n",
    "\n",
    "    # Compute centroids for each language\n",
    "    language_centroids = {}\n",
    "    for language, samples in training_data_by_language.items():\n",
    "        vectors = []\n",
    "        for sample in samples:\n",
    "            ngram_vectors = generate_encoded_ngrams(samples, n, d, char_to_hd)\n",
    "            # if the vector contains NaN values, print the language and the centroid, do not include it\n",
    "            if(np.isnan(ngram_vectors).any()):\n",
    "                print('Error')\n",
    "                print(language)\n",
    "                print(ngram_vectors[0])\n",
    "                continue\n",
    "            vectors.extend(ngram_vectors)\n",
    "        language_centroids[language] = compute_language_centroid(vectors)\n",
    "        if(np.isnan(language_centroids[language]).any()):\n",
    "            print('Error')\n",
    "            print(language)\n",
    "            print(language_centroids[language])\n",
    "\n",
    "    print(\"Centroids computed for each language\")\n",
    "\n",
    "    # Classify and evaluate on the test set\n",
    "    y_pred = []\n",
    "    for sample, predicted_label in zip(X_test, y_test):\n",
    "        query_vector = np.mean(generate_encoded_ngrams(sample, n, d, char_to_hd), axis=0)\n",
    "        predicted_label = classify_text(query_vector, language_centroids)\n",
    "        y_pred.append(predicted_label)\n",
    "\n",
    "    # Evaluation with proper labels parameter\n",
    "    labels_set = list(language_centroids.keys())\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, labels=labels_set))\n",
    "\n",
    "\n",
    "    # Assuming you have your y_true (test_labels) and y_pred (y_pred_europarl)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels_set)\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.set(font_scale=1.2)\n",
    "    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                    xticklabels=labels_set, yticklabels=labels_set, linewidths=0.5, linecolor='gray')\n",
    "\n",
    "    # Add labels, title and axis ticks\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.xlabel('Predicted Labels', fontsize=12)\n",
    "    plt.ylabel('True Labels', fontsize=12)\n",
    "\n",
    "    # Rotate the tick labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
